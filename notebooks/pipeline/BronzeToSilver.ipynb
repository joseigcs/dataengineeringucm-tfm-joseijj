{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Bronze to silver notebook\n",
        "\n",
        "En esta notebook, los datos financieros que fueron inicialmente almacenados en la capa Bronze son leídos y sometidos a una serie de transformaciones, tales como la limpieza de los datos, la imputación de valores nulos y la creación de nuevas columnas. \n",
        "\n",
        "## Parámetros\n",
        "- **Asset (str)**: Indica el activo financiero que se desea analizar. Es el nombre del directorio del que se recibe la información delta y en el que se guardará. Es el nombre del directorio en el que se guardará. 'apple' es el valor por defecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Paramaters\n",
        "asset = 'apple' "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import sys\r\n",
        "import pandas as pd\r\n",
        "from delta.tables import DeltaTable \r\n",
        "from pyspark.sql import DataFrame \r\n",
        "from pyspark.sql.functions import col, last, max, min, year, month, dayofmonth, dayofweek, hour\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from typing import List\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "def valida_nulos(df: DataFrame, lista_columnas: list[str]) -> None:\r\n",
        "    \"\"\"\r\n",
        "    Valida que no existan valores nulos en las columnas especificadas del DataFrame.\r\n",
        "\r\n",
        "    Esta función recorre una lista de columnas y verifica si hay valores nulos\r\n",
        "    en cada una de ellas. Si se encuentran valores nulos en alguna columna de la lista, se lanza una\r\n",
        "    excepción con un mensaje que indica la columna en la que se encontraron los valores nulos.\r\n",
        "\r\n",
        "    Parámetros:\r\n",
        "    df (DataFrame): El DataFrame de Spark que contiene los datos a verificar.\r\n",
        "    lista_columnas (list[str]): LA lista de nombres de columnas que se desean verificar.\r\n",
        "\r\n",
        "    Excepciones:\r\n",
        "    AssertionError: Si se encuentran valores nulos en alguna de las columnas especificadas,\r\n",
        "                    se lanza una excepción con un mensaje indicando la columna con valores nulos.\r\n",
        "    \"\"\"\r\n",
        "    for i in lista_columnas:\r\n",
        "        conteo_nulos = df.filter(df[i].isNull()).count()\r\n",
        "        assert conteo_nulos == 0, f\"Existen valores nulos en la columna {i}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "storage_account_name = 'nticmasterstg' \r\n",
        " \r\n",
        "data_lake_container = f'abfss://datalake@{storage_account_name}.dfs.core.windows.net' \r\n",
        "bronze_folder = 'bronze'  # Carpeta de ingesta de datos (raw) \r\n",
        "silver_folder = 'silver'  # Carpeta donde se almacenarán las tablas resultantes\r\n",
        "\r\n",
        "# Ruta de los archivos de origen \r\n",
        "source_path = f\"{data_lake_container}/{bronze_folder}/{asset}\" \r\n",
        " \r\n",
        "# Ruta de la tabla Delta de guardado\r\n",
        "delta_table_path = f\"{data_lake_container}/{silver_folder}/{asset}\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Carga de datos de entrada\n",
        "data_input = spark.read.format('delta').option(\"recursiveFileLookup\", \"true\").option(\"header\", \n",
        "\"true\").load(source_path) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Comprobaciones de datos\n",
        "\n",
        "# Mostrar las primeras filas del DataFrame\n",
        "print(\"Primeras filas del DataFrame:\")\n",
        "print(data_input.head())\n",
        "\n",
        "# Verificar la columna de fechas\n",
        "print(\"\\nColumnas del DataFrame:\")\n",
        "print(data_input.columns)\n",
        "\n",
        "# Comprobar el formato de la columna de fechas\n",
        "print(\"\\nTipo de datos de la columna 'Date':\")\n",
        "print(data_input['Date'].dtype)\n",
        "\n",
        "# Mostrar el rango de fechas disponibles\n",
        "date_range = data_input.select(min('Date').alias('min_date'), max('Date').alias('max_date')).collect()\n",
        "\n",
        "# Mostrar el rango de fechas disponibles\n",
        "min_date = date_range[0]['min_date']\n",
        "max_date = date_range[0]['max_date']\n",
        "print(f\"\\nRango de fechas disponibles: {min_date} al {max_date}\")\n",
        "\n",
        "# Mostrar la primera fila del dataframe\n",
        "print(\"\\nPrimeras filas del DataFrame:\")\n",
        "print(data_input.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n",
        "silver_df = data_input.withColumn(\"Date\", col(\"Date\").cast(\"timestamp\"))\n",
        "\n",
        "# Eliminamos duplicados respecto a las columnas 'Date' y 'Close'.\n",
        "silver_df = silver_df.dropDuplicates(['Date', 'Close'])\n",
        "\n",
        "# Creamos nuevas columnas basadas en la fecha\n",
        "silver_df = silver_df.withColumn(\"year\", year(col(\"Date\")))\n",
        "silver_df = silver_df.withColumn(\"month\", month(col(\"Date\")))\n",
        "silver_df = silver_df.withColumn(\"day_of_month\", dayofmonth(col(\"Date\")))\n",
        "silver_df = silver_df.withColumn(\"day_of_week\", dayofweek(col(\"Date\")))\n",
        "\n",
        "# Imputamos valores nulos, el volumen de movimiento lo sustituimos por 0, mientras que el resto de valores utilizamos el último valor disponible (forward fill)\n",
        "silver_df = silver_df.fillna({\"Volume\": 0}) \n",
        "window_spec = Window.orderBy('Date').rowsBetween(-sys.maxsize, 0)\n",
        "\n",
        "columns_to_ffill = ['Open', 'High', 'Low', 'Close', 'Adj_Close']\n",
        "\n",
        "for column in columns_to_ffill:\n",
        "    silver_df = silver_df.withColumn(column, last(silver_df[column], ignorenulls=True).over(window_spec))\n",
        "\n",
        "# Estandarizamos formatos de datos \n",
        "silver_df = silver_df.withColumn(\"Open\", col(\"Open\").cast(\"double\"))\n",
        "silver_df = silver_df.withColumn(\"High\", col(\"High\").cast(\"double\"))\n",
        "silver_df = silver_df.withColumn(\"Low\", col(\"Low\").cast(\"double\"))\n",
        "silver_df = silver_df.withColumn(\"Close\", col(\"Close\").cast(\"double\"))\n",
        "silver_df = silver_df.withColumn(\"Adj_Close\", col(\"Adj_Close\").cast(\"double\"))\n",
        "\n",
        "# Lista de columnas a verificar\n",
        "columnas_a_revisar = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj_Close', 'Volume']\n",
        "\n",
        "# Llamada a la función para verificar nulos en las columnas especificadas\n",
        "valida_nulos(silver_df, columnas_a_revisar)\n",
        "\n",
        "# Filtrar y eliminar registros correspondientes a sábados y domingos\n",
        "silver_df = silver_df.filter(~col(\"day_of_week\").isin([6, 7]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "silver_df.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "\n",
        "print('La tabla {table_name} se ha guardado en la ruta {delta_table_path}')\n",
        "\n",
        "delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "print('Mostrando 5 líneas del contenido de la tabla Delta...')\n",
        "delta_df.show(5)\n",
        "\n",
        "conteo_filas = delta_df.count()\n",
        "\n",
        "print(f'La tabla Delta tiene {conteo_filas} filas en total')"
      ]
    }
  ]
}