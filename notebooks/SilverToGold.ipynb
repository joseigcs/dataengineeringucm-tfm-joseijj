{
  "nbformat": 4,
  "nbformat_minor": 2,
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "python"
    },
    "description": null,
    "save_output": true,
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Silver to gold notebook\n",
        "\n",
        "En esta última notebook, los datos transformados y limpios provenientes de la capa Silver son leídos y sometidos a una serie de transformaciones adicionales para preparar la información en la capa Gold. El objetivo de esta etapa es enriquecer los datos y calcular métricas financieras avanzadas que facilitarán los análisis posteriores. Esta notebook debe ser ejecutada en Azure Synapse.\n",
        "\n",
        "## Parámetros\n",
        "- **Asset (str)**: Indica el activo financiero que se desea analizar. Es el nombre del directorio del que se recibe la información delta y en el que se guardará. Es el nombre del directorio en el que se guardará. 'apple' es el valor por defecto."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "tags": [
          "parameters"
        ]
      },
      "source": [
        "# Paramaters\n",
        "asset = 'apple'     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as F\r\n",
        "from pyspark.sql.window import Window\r\n",
        "from pyspark.sql.functions import last, col"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Definir variables  \n",
        "storage_account_name = 'nticmasterstg' \n",
        "data_lake_container = f'abfss://datalake@{storage_account_name}.dfs.core.windows.net' \n",
        "silver_folder = 'silver'  # Carpeta de entrada (en este caso silver)\n",
        "gold_folder = 'gold' # Directorio final\n",
        "\n",
        "# Determinar la ruta de los archivos de origen \n",
        "source_path_asset = f\"{data_lake_container}/{silver_folder}/{asset}\" \n",
        " \n",
        "# Determinar la ruta de la tabla final en gold\n",
        "delta_table_path = f\"{data_lake_container}/{gold_folder}/{asset}\" \n",
        "\n",
        "# Lectura de los datos\n",
        "data_input = spark.read.format('delta').option(\"recursiveFileLookup\", \"true\").option(\"header\", \n",
        "\"true\").load(source_path_asset) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Carga de la tabla EPU\r\n",
        "source_path_epu = f\"{data_lake_container}/{gold_folder}/epu_data\"\r\n",
        "\r\n",
        "epu_data = df = spark.read.parquet(source_path_epu)\r\n",
        "\r\n",
        "epu_data = epu_data.withColumnRenamed('Year', 'year').withColumnRenamed('Month', 'month')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Calcular la media móvil de 7 días\n",
        "data_gold = data_input.withColumn(\n",
        "    '7_day_moving_avg', \n",
        "    F.avg('Adj_Close').over(Window.orderBy('Date').rowsBetween(-6, 0))\n",
        ")\n",
        "\n",
        "# Calcular la media móvil de 30 días\n",
        "data_gold = data_gold.withColumn(\n",
        "    '30_day_moving_avg', \n",
        "    F.avg('Adj_Close').over(Window.orderBy('Date').rowsBetween(-29, 0))\n",
        ")\n",
        "\n",
        "# Calcular la variación diaria del precio de cierre ajustado\n",
        "data_gold = data_gold.withColumn(\n",
        "    'daily_return', \n",
        "    (F.col('Adj_Close') - F.lag('Adj_Close', 1).over(Window.orderBy('Date'))) / F.lag('Adj_Close', 1).over(Window.orderBy('Date'))\n",
        ")\n",
        "\n",
        "# Agrupar por año y mes para calcular métricas mensuales\n",
        "monthly_aggregates = data_gold.groupBy('year', 'month').agg(\n",
        "    F.first('Open').alias('monthly_open'),\n",
        "    F.max('High').alias('monthly_high'),\n",
        "    F.min('Low').alias('monthly_low'),\n",
        "    F.last('Close').alias('monthly_close'),\n",
        "    F.sum('Volume').alias('monthly_volume')\n",
        ")\n",
        "\n",
        "data_gold = data_gold.join(\n",
        "    monthly_aggregates, \n",
        "    on=['year', 'month'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Join con la info de EPU\n",
        "data_gold = data_gold.join(\n",
        "    epu_data, \n",
        "    on=['year', 'month'], \n",
        "    how='left'\n",
        ")\n",
        "\n",
        "data_gold.show(5)\n",
        "\n",
        "data_gold.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
        "\n",
        "print('La tabla {asset} se ha guardado en la ruta {delta_table_path}')\n",
        "\n",
        "delta_df = spark.read.format(\"delta\").load(delta_table_path)\n",
        "print('Mostrando 5 líneas del contenido de la tabla Delta...')\n",
        "delta_df.show(5)\n",
        "\n",
        "conteo_filas = delta_df.count()\n",
        "\n",
        "print(f'La tabla Delta tiene {conteo_filas} filas en total')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "spark.sql('CREATE DATABASE IF NOT EXISTS Gold') \r\n",
        "# Añadir la tabla Delta a la base de datos Gold para facilitar consultas y conectar con PowerBI\r\n",
        "spark.sql(f\"CREATE TABLE IF NOT EXISTS Gold.{asset} USING DELTA LOCATION '{delta_table_path}'\")\r\n",
        "\r\n",
        "df = spark.sql(f'SELECT * FROM Gold.{asset} LIMIT 5')\r\n",
        "df.show()\r\n",
        ""
      ]
    }
  ]
}